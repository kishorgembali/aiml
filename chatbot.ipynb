{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue\">Electronics & ICT Academy National Institute of Technology, Warangal</h2>\n",
    "\n",
    "### Course: Post Graduate Program in Machine Learning and Artificial Intelligence\n",
    "#### Project: Building A Conversational Chatbot\n",
    "\n",
    "<p>Aim: Aim of the project is to build an intelligent conversational chatbot, Riki, that can understand complex queries from the user and intelligently respond.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download GloVe Model\n",
    "\n",
    "In this task, download the GloVe model provided by Stanford NLP. The specific model will download is the Twitter version with the following specifications: 2 billion tweets, 27 billion tokens, 1.2 million vocabulary, uncased, and available in various dimensions (25d, 50d, 100d, and 200d vectors). The download size is approximately 1.42 GB.\n",
    "\n",
    "#### Steps to Download\n",
    "\n",
    "Follow these steps to download the GloVe Twitter model:\n",
    "\n",
    "1. *Access the Download Page:*\n",
    "   - Visit the [Stanford NLP GloVe Project](https://nlp.stanford.edu/projects/glove/) page.\n",
    "\n",
    "2. *Locate the Twitter Model:*\n",
    "   - On the project page, scroll down until you find the section for the Twitter models.\n",
    "\n",
    "3. *Download the Zip File:*\n",
    "   - Click on the download link for models. For example, click on the link for `glove.twitter.27B.zip`.\n",
    "\n",
    "4. *Wait for Download:*\n",
    "   - Depending on your internet connection, the download may take some time due to the large size of the model.\n",
    "\n",
    "5. *Extract the Model:*\n",
    "   - Once the download is complete, extract the contents of the downloaded ZIP file to access the GloVe model files.\n",
    "\n",
    "6. *Use the GloVe Model:*\n",
    "   - Now, use the GloVe model in our NLP project for tasks such as word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GloVe Word Embeddings into a Dictionary\n",
    "\n",
    "In this task, demonstrate how to load GloVe word embeddings into a Python dictionary, where each unique word token serves as the key, and the associated value is a d-dimensional vector. This allows you to efficiently access word vectors for natural language processing tasks.\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "Before proceeding, make sure you have:\n",
    "\n",
    "- Downloaded the GloVe model as described in the previous Markdown document, [Download GloVe Model](#link-to-download-glove-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe word embeddings (adjust the path and dimension as needed)\n",
    "glove_file = 'glove_model/glove.twitter.27B.100d.txt'\n",
    "embedding_dim = 100\n",
    "\n",
    "def load_glove_embeddings(file_path, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_embeddings_index = load_glove_embeddings(glove_file, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In this step, prepare the dataset for training a dialogue model. This involves:\n",
    "\n",
    "1. Filtering the conversations to a maximum word length.\n",
    "2. Converting the dialogue pairs into input text and target text.\n",
    "3. Adding special start and end tokens to recognize the beginning and end of each sentence.\n",
    "\n",
    "Let's proceed with these steps to ensure the data is ready for training our dialogue model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Conversations and Lines from Files\n",
    "def load_conversations():\n",
    "    conversations = []\n",
    "    with open('cornell_movie_dialogs_corpus/movie_conversations.txt', 'r', encoding='utf-8', errors='ignore') as conv_file:\n",
    "        for line in conv_file:\n",
    "            conversation = line.strip().split(\" +++$+++ \")[-1][1:-1].replace(\"'\", \"\").split(', ')\n",
    "            conversations.append(conversation)\n",
    "    return conversations\n",
    "\n",
    "def load_lines():\n",
    "    lines = {}\n",
    "    with open('cornell_movie_dialogs_corpus/movie_lines.txt', 'r', encoding='utf-8', errors='ignore') as lines_file:\n",
    "        for line in lines_file:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            lines[parts[0]] = parts[-1]\n",
    "    return lines\n",
    "\n",
    "# Filter Conversations by Maximum Word Length\n",
    "def filter_conversations(conversations, max_length):\n",
    "    filtered_conversations = []\n",
    "    for conversation in conversations:\n",
    "        if all(len(re.findall(r'\\w+', line)) <= max_length for line in conversation):\n",
    "            filtered_conversations.append(conversation)\n",
    "    return filtered_conversations\n",
    "\n",
    "# Convert Dialogue Pairs into Input and Target Texts\n",
    "def convert_to_input_target(conversations, lines):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation) - 1):\n",
    "            input_line_id = conversation[i]\n",
    "            target_line_id = conversation[i + 1]\n",
    "            \n",
    "            input_text = lines.get(input_line_id, \"\")  # Get the text associated with the line ID\n",
    "            target_text = lines.get(target_line_id, \"\")  # Get the text associated with the line ID\n",
    "            \n",
    "            if input_text and target_text:\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "    return input_texts, target_texts\n",
    "\n",
    "# Add Start and End Tokens\n",
    "def add_start_end_tokens(texts):\n",
    "    start_token = '<start>'\n",
    "    end_token = '<end>'\n",
    "    return [f'{start_token} {text} {end_token}' for text in texts]\n",
    "\n",
    "# Define your max_word_length\n",
    "max_word_length = 15  # Adjust as needed\n",
    "\n",
    "# Load conversations and lines from the dataset\n",
    "conversations = load_conversations()\n",
    "lines = load_lines()\n",
    "\n",
    "# Filter conversations by maximum word length\n",
    "filtered_conversations = filter_conversations(conversations, max_word_length)\n",
    "\n",
    "# Convert dialogue pairs into input and target texts\n",
    "input_texts, target_texts = convert_to_input_target(filtered_conversations, lines)\n",
    "\n",
    "# Add start and end tokens to target texts\n",
    "target_texts = add_start_end_tokens(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to train the model\n",
    "sorted_input_texts = []\n",
    "sorted_target_texts = []\n",
    "\n",
    "for i in range(len(input_texts)):\n",
    "    if (len(input_texts[i]) > 10) and (len(input_texts[i]) < 50)and (len(target_texts[i]) > 20) and (len(target_texts[i]) < 60):\n",
    "        sorted_input_texts.append(input_texts[i])\n",
    "        sorted_target_texts.append(target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionaries and Save to Disk\n",
    "\n",
    "In this step, create two dictionaries:\n",
    "1. `target_word2id`: A dictionary where the keys are unique target words, and the values are their corresponding IDs.\n",
    "2. `target_id2word`: A dictionary where the keys are target word IDs, and the values are the corresponding words.\n",
    "\n",
    "Save these dictionaries in the NumPy file format on the disk for future use.\n",
    "\n",
    "Let's proceed with creating and saving these dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target_word2id and target_id2word dictionaries\n",
    "target_word2id = {}\n",
    "target_id2word = {}\n",
    "target_words = list(set(word for text in sorted_target_texts for word in text.split()))\n",
    "\n",
    "for idx, word in enumerate(target_words):\n",
    "    target_word2id[word] = idx\n",
    "    target_id2word[idx] = word\n",
    "\n",
    "# Size of the vocabulary\n",
    "vocab_size = len(target_word2id)\n",
    "\n",
    "# Convert dictionaries to NumPy arrays\n",
    "target_word2id_array = np.array(list(target_word2id.items()), dtype=object)\n",
    "target_id2word_array = np.array(list(target_id2word.items()), dtype=object)\n",
    "\n",
    "# Define the file paths to save the NumPy arrays\n",
    "target_word2id_file = 'target_word2id.npy'\n",
    "target_id2word_file = 'target_id2word.npy'\n",
    "\n",
    "# Save the NumPy arrays to disk\n",
    "np.save(target_word2id_file, target_word2id_array)\n",
    "np.save(target_id2word_file, target_id2word_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the structure of the input data, which consists of a list of sentences. \n",
    "# Each sentence is itself represented as a list of words.\n",
    "\n",
    "# Tokenize sentences for Inputs\n",
    "tokenizer_input = Tokenizer(filters=\"\")\n",
    "tokenizer_input.fit_on_texts(sorted_input_texts)  # input_texts contains your preprocessed sentences\n",
    "\n",
    "# Map words to GloVe embeddings\n",
    "num_words = len(tokenizer_input.word_index) + 1  # Add 1 for the padding token\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in tokenizer_input.word_index.items():\n",
    "    embedding_vector = glove_embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Convert sentences to sequences of word indices\n",
    "input_sequences = tokenizer_input.texts_to_sequences(sorted_input_texts)\n",
    "\n",
    "# Pad input sequences to a fixed length (you can adjust the maxlen)\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen=20, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer_input to a file\n",
    "with open('tokenizer_input.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer_input, tokenizer_file)\n",
    "    \n",
    "# Load tokenizer_input from a file\n",
    "# with open('tokenizer_input.pkl', 'rb') as tokenizer_file:\n",
    "#     tokenizer_input = pickle.load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences for Outputs\n",
    "tokenizer_target = Tokenizer(filters=\"\")\n",
    "tokenizer_target.fit_on_texts(sorted_target_texts)\n",
    "\n",
    "Map words to GloVe embeddings for targets\n",
    "for word, i in tokenizer_target.word_index.items():\n",
    "    embedding_vector = glove_embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Convert sentences to sequences of word indices\n",
    "target_sentences = [s.split() for s in sorted_target_texts]  # Split target sentences into words\n",
    "target_sequences = tokenizer_target.texts_to_sequences(target_sentences)\n",
    "\n",
    "# Pad input sequences to a fixed length (you can adjust the maxlen)\n",
    "padded_target_sequences = pad_sequences(target_sequences, maxlen=20, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer_target to a file\n",
    "with open('tokenizer_target.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer_target, tokenizer_file)\n",
    "    \n",
    "# Load tokenizer_target from a file\n",
    "# with open('tokenizer_target.pkl', 'rb') as tokenizer_file:\n",
    "#     tokenizer_target = pickle.load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the One hot encoding for the target sequences to use during the model training\n",
    "num_target_tokens = len(tokenizer_target.word_index) + 1  # +1 for padding\n",
    "target_sequences_one_hot = to_categorical(padded_target_sequences, num_classes=num_target_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Training Data per Batch\n",
    "\n",
    "In this step, training data will be generated in batches.\n",
    "\n",
    "The dataset will be divided into smaller batches. During each training iteration, one batch of data will be fed to the model for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_generator(x, y, batch_size):\n",
    "    \"\"\"Generates batches of vectorized texts for training/validation.\n",
    "\n",
    "    # Arguments\n",
    "        x: np.matrix, feature matrix.\n",
    "        y: np.ndarray, labels.\n",
    "        num_features: int, number of features.\n",
    "        batch_size: int, number of samples per batch.\n",
    "\n",
    "    # Returns\n",
    "        Yields feature and label data in batches.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size:\n",
    "        num_batches += 1\n",
    "\n",
    "    while 1:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            if end_idx > num_samples:\n",
    "                end_idx = num_samples\n",
    "            x_batch = x[start_idx:end_idx]\n",
    "            y_batch = y[start_idx:end_idx]\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "# Create training and validation generators.\n",
    "training_generator = _data_generator(\n",
    "    padded_input_sequences, padded_target_sequences, 32)\n",
    "\n",
    "# Get number of training steps. This indicated the number of steps it takes\n",
    "# to cover all samples in one epoch.\n",
    "steps_per_epoch = padded_input_sequences.shape[0] // 32\n",
    "if padded_input_sequences.shape[0] % 32:\n",
    "    steps_per_epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Overview\n",
    "\n",
    "This section explains how our model is structured for sequence-to-sequence tasks:\n",
    "\n",
    "#### Step 1: LSTM Encoder\n",
    "- Use an LSTM encoder to understand the input words. It generates information about the input, including encoder outputs, encoder hidden state, and encoder context.\n",
    "\n",
    "#### Step 2: LSTM Decoder\n",
    "- Use an LSTM decoder helps generate target words. It relies on the information from the encoder and produces decoder outputs, decoder hidden state, and decoder context.\n",
    "\n",
    "#### Step 3: Prediction with Dense Layer\n",
    "- To predict the next word from our vocabulary, we use a dense layer. This step is crucial for creating meaningful sequences.\n",
    "\n",
    "#### Step 4: Loss and Optimization\n",
    "- For training the model, choose the 'categorical_crossentropy' loss function and the 'rmsprop' optimizer. This helps improve the model's accuracy during training.\n",
    "\n",
    "Following these steps, model is set up to learn and generate sequences effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_seq_length = 20 #max(len(seq) for seq in source_sequences)\n",
    "max_target_seq_length = 20 #max(len(seq) for seq in target_sequences)\n",
    "\n",
    "# Define the model\n",
    "latent_dim = 256  # Dimensionality of the LSTM hidden state\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_source_seq_length,))\n",
    "encoder_embedding = Embedding(\n",
    "    input_dim=len(tokenizer_source.word_index),\n",
    "    output_dim=100,  # Use the same dimension as GloVe embeddings (100d)\n",
    "    weights=[np.array([glove_embeddings_index[word] if word in glove_embeddings_index else np.zeros(100) for word in tokenizer_source.word_index.keys()])],\n",
    "    trainable=False,  # Freeze the embeddings\n",
    "    input_length=max_source_seq_length,\n",
    ")(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_target_seq_length,))\n",
    "decoder_embedding = Embedding(input_dim=num_target_tokens, output_dim=latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_target_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary\n",
    "\n",
    "In this section, an overview of the model's structure and parameters is presented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "history = model.fit(\n",
    "    [padded_input_sequences, padded_target_sequences],\n",
    "    target_sequences_one_hot,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('chatbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Predictions\n",
    "\n",
    "In this final step, an operational model generates predictions. After completing the training and setup phases, this is where the model's learned knowledge is applied. It takes input data and produces predictions based on its training.\n",
    "\n",
    "Let's proceed to generate predictions using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder inputs, embedding, and LSTM layers\n",
    "encoder_inputs = model.input[0]  # Input layer for encoder\n",
    "decoder_inputs = model.input[1]\n",
    "encoder_embedding = model.layers[2]  # Embedding layer for encoder\n",
    "encoder_lstm = model.layers[4]  # LSTM layer for encoder\n",
    "\n",
    "# Define the encoder model for inference\n",
    "encoder_outputs, state_h_enc, state_c_enc = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define the decoder inputs and states\n",
    "decoder_lstm = model.layers[5]  # LSTM layer for decoder\n",
    "decoder_states_inputs = [\n",
    "    keras.Input(shape=(latent_dim,)),  # Initial state for LSTM (state_h)\n",
    "    keras.Input(shape=(latent_dim,))   # Initial state for LSTM (state_c)\n",
    "]\n",
    "\n",
    "# Define the decoder embedding layer (should match the architecture)\n",
    "decoder_embedding = model.layers[3]  # Embedding layer for decoder\n",
    "\n",
    "# Connect the decoder LSTM to the inputs and states\n",
    "decoder_outputs_and_states = decoder_lstm(\n",
    "    decoder_embedding(decoder_inputs),  # Embedding layer for decoder\n",
    "    initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_outputs_and_states\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "\n",
    "# Define the decoder dense layer (should match the architecture)\n",
    "decoder_dense = model.layers[6]  # Dense layer for decoder\n",
    "\n",
    "# Connect the dense layer to the decoder LSTM\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the decoder model for inference\n",
    "decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(input_sentence):\n",
    "    input_seq = tokenizer_source.texts_to_sequences([input_sentence])[0]\n",
    "    input_seq = pad_sequences([input_seq], maxlen=max_source_seq_length, padding='post')\n",
    "\n",
    "    initial_states = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1))  # Initialize the target sequence with a start token\n",
    "    target_text = ''\n",
    "\n",
    "    while True:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + initial_states)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "\n",
    "        if sampled_token_index == 0:\n",
    "            break\n",
    "        sampled_word = tokenizer_target.index_word[sampled_token_index]\n",
    "\n",
    "        if sampled_word == '<end>' or len(target_text.split()) >= max_target_seq_length:\n",
    "            break\n",
    "\n",
    "        if sampled_word != \"<start>\":\n",
    "            target_text += sampled_word + ' '\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        initial_states = [h, c]\n",
    "\n",
    "    return target_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the translation function\n",
    "input_sentence = \"Hello\"\n",
    "predicted_sentence = predict_sentence(input_sentence)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Predicted Sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the translation function\n",
    "input_sentence = \"i am not feeling well.\"\n",
    "predicted_sentence = predict_sentence(input_sentence)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Predicted Sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the translation function\n",
    "input_sentence = \"Sorry.\"\n",
    "predicted_sentence = predict_sentence(input_sentence)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Predicted Sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
